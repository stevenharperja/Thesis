- Word2Vec
    - [run_word2vec](https://radimrehurek.com/gensim/auto_examples/tutorials/run_word2vec.html) 
        - [The Skip-Gram Model](https://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/)
            - Apparently word2vec can help handle stemming (cutting off grammar tense) automatically!
            - [Part 2](https://mccormickml.com/2017/01/11/word2vec-tutorial-part-2-negative-sampling/)
                - [Word2vec applied to non-sentence data!](https://mccormickml.com/2018/06/15/applying-word2vec-to-recommenders-and-advertising/)
                - So, from my understanding:
                    - In implementation, you make a 2-layer nn with an identity function followed by a softmax, to predict surrounding words. then you extract the 1st layer's weights. That's it.
                    - Mathematically though, this creates a sort of "Given X, what is the prob of Y?" matrix, however, it is, at max, the rank of your hidden layer size, which forces it to be a dense matrix rather than a sparse matrix. And it is *almost* linearly related to a hypothetical X->Y matrix, however its far more useful because it compresses so much information into so (relatively) few dimensions.
    - NLP aug has a function for using gensim models. So we just need to figure out gensim. https://github.com/makcedward/nlpaug
    - Left off on "The Word2Vec Skip-gram model, for example, takes in pairs (word1, word2) generated by "